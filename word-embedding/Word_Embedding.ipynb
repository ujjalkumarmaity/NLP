{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d7e1869",
      "metadata": {
        "id": "0d7e1869"
      },
      "source": [
        "## Word Embedding\n",
        "- Word embedding is a technique used in natural language processing (NLP) to **represent words as dense vectors in a continuous space**. For example, we can easily understand the text \"I saw a cat\", but our models can not - they need vectors of features. Such vectors, or word embeddings, are representations of words which can be fed into your model.\n",
        "- It helps capture the semantic, syntactic context or a word/term and helps understand how similar/dissimilar it is to other word.\n",
        "\n",
        "Advantage of word embedding\n",
        "- sementic similarity\n",
        "- conextual understanding\n",
        "- Dimentionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56058cf",
      "metadata": {
        "id": "c56058cf"
      },
      "source": [
        "## Word Embedding techniques\n",
        "- BOW Embedding\n",
        "- TF-IDF Embedding\n",
        "- Word2Vec Embedding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc2ac69e",
      "metadata": {
        "id": "fc2ac69e"
      },
      "source": [
        "### Bag Of Word(BOW)\n",
        "- It convert text document into numerical vector representation\n",
        "- BoW model **ignores the order and structure of the words in the document** and focuses solely on the **occurrence and frequency of words**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac19fb8a",
      "metadata": {
        "id": "ac19fb8a",
        "outputId": "98961983-7814-4b4c-ad32-e6bf62995358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['and' 'bow' 'convert' 'document' 'focuses' 'frequency' 'ignores' 'in'\n",
            " 'into' 'it' 'model' 'numerical' 'occurrence' 'of' 'on' 'order'\n",
            " 'representation' 'solely' 'structure' 'text' 'the' 'vector' 'words']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>and</th>\n",
              "      <th>bow</th>\n",
              "      <th>convert</th>\n",
              "      <th>document</th>\n",
              "      <th>focuses</th>\n",
              "      <th>frequency</th>\n",
              "      <th>ignores</th>\n",
              "      <th>in</th>\n",
              "      <th>into</th>\n",
              "      <th>it</th>\n",
              "      <th>...</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>order</th>\n",
              "      <th>representation</th>\n",
              "      <th>solely</th>\n",
              "      <th>structure</th>\n",
              "      <th>text</th>\n",
              "      <th>the</th>\n",
              "      <th>vector</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   and  bow  convert  document  focuses  frequency  ignores  in  into  it  \\\n",
              "0    0    0        1         1        0          0        0   0     1   1   \n",
              "1    3    1        0         1        1          1        1   1     0   0   \n",
              "\n",
              "   ...  of  on  order  representation  solely  structure  text  the  vector  \\\n",
              "0  ...   0   0      0               1       0          0     1    0       1   \n",
              "1  ...   2   1      1               0       1          1     0    4       0   \n",
              "\n",
              "   words  \n",
              "0      0  \n",
              "1      2  \n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "texts = ['It convert text document into numerical vector representation',\n",
        "        'BoW model ignores the order and structure of the words in the document and focuses solely on the occurrence and frequency of words']\n",
        "texts = [i.lower() for i in texts]\n",
        "cv = CountVectorizer() # initilize CountVectorizer model\n",
        "cv.fit(texts)\n",
        "cv_vector = cv.fit_transform(texts)\n",
        "featrure = cv.get_feature_names_out()\n",
        "print(featrure) #list of word\n",
        "pd.DataFrame(cv_vector.toarray(),columns = featrure)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf5fe23",
      "metadata": {
        "id": "caf5fe23"
      },
      "source": [
        "### TF-IDF ( Term Frequency-Inverse Document Fequency)\n",
        "- It is used for evulate improtance of a term in a document within a collection or corpus of documents\n",
        "- **Term Frequency** - It measure how frequent a term appear in a document. TF can help identify important terms within a document by giving higher weight to terms that occur more frequently.<br>\n",
        "TF($w_i,r_j$) = (Number of time $w_i$ occure in $r_j$) / (Total number of word in $r_j$)\n",
        "\n",
        "- **Inverse Document Fequency** - It measure improtance of a word across the entire corpus. It penalizes terms that appear in many documents and gives more weight to terms that are relatively rare.<br>\n",
        "IDF = log((Total number of documents) / (Number of documents containing the term))<br>\n",
        "TF-IDF = TF * IDF\n",
        "\n",
        "#### Limitation -\n",
        "- Semantic Understanding - TF-IDF does not capture sementic meanng of words\n",
        "- Vocabulary Size and Sparsity\n",
        "- Word Importance Assumption: TF-IDF assume improtance of a word directly proportional to its frequency. But sometime rear fequent word also more improtant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf14e21",
      "metadata": {
        "id": "cdf14e21",
        "outputId": "5ebfaca6-0792-4b99-e5b5-04985223b047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['and' 'bow' 'convert' 'document' 'focuses' 'frequency' 'ignores' 'in'\n",
            " 'into' 'it' 'model' 'numerical' 'occurrence' 'of' 'on' 'order'\n",
            " 'representation' 'solely' 'structure' 'text' 'the' 'vector' 'words']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>and</th>\n",
              "      <th>bow</th>\n",
              "      <th>convert</th>\n",
              "      <th>document</th>\n",
              "      <th>focuses</th>\n",
              "      <th>frequency</th>\n",
              "      <th>ignores</th>\n",
              "      <th>in</th>\n",
              "      <th>into</th>\n",
              "      <th>it</th>\n",
              "      <th>...</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>order</th>\n",
              "      <th>representation</th>\n",
              "      <th>solely</th>\n",
              "      <th>structure</th>\n",
              "      <th>text</th>\n",
              "      <th>the</th>\n",
              "      <th>vector</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>0.259698</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364996</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.449687</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106652</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.299792</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.149896</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.599583</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.299792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        and       bow   convert  document   focuses  frequency   ignores  \\\n",
              "0  0.000000  0.000000  0.364996  0.259698  0.000000   0.000000  0.000000   \n",
              "1  0.449687  0.149896  0.000000  0.106652  0.149896   0.149896  0.149896   \n",
              "\n",
              "         in      into        it  ...        of        on     order  \\\n",
              "0  0.000000  0.364996  0.364996  ...  0.000000  0.000000  0.000000   \n",
              "1  0.149896  0.000000  0.000000  ...  0.299792  0.149896  0.149896   \n",
              "\n",
              "   representation    solely  structure      text       the    vector     words  \n",
              "0        0.364996  0.000000   0.000000  0.364996  0.000000  0.364996  0.000000  \n",
              "1        0.000000  0.149896   0.149896  0.000000  0.599583  0.000000  0.299792  \n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "texts = ['It convert text document into numerical vector representation',\n",
        "        'BoW model ignores the order and structure of the words in the document and focuses solely on the occurrence and frequency of words']\n",
        "texts = [i.lower() for i in texts]\n",
        "tfidf = TfidfVectorizer() # initilize CountVectorizer model\n",
        "tfidf.fit(texts)\n",
        "tfidf_vector = tfidf.fit_transform(texts)\n",
        "featrure = tfidf.get_feature_names_out()\n",
        "print(featrure) #list of word\n",
        "pd.DataFrame(tfidf_vector.toarray(),columns = featrure)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a9f8e54",
      "metadata": {
        "id": "1a9f8e54"
      },
      "source": [
        "### Word2Vec\n",
        "-  As name suggest it convert word to vector. It's aims capture sementic and syntactic relationship between words by mapping them into a continuous vector space.\n",
        "- Two main archicture, they are- Continuous Bag-of-Words (CBOW) and Skip-gram. Both architectures involve training a neural network on a large corpus of text data and help the network learn how to represent a word. This is **unsupervised machine learning, and labels are needed to train the model**.\n",
        "<img height = 300 width = 500  src = 'https://miro.medium.com/v2/resize:fit:1100/0*dyZ7Syt3DMbN7nF9'>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ed281d",
      "metadata": {
        "id": "e4ed281d"
      },
      "source": [
        "**TO bo added**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9664bcd8",
      "metadata": {
        "id": "9664bcd8"
      },
      "source": [
        "Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
        "\n",
        "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
        "\n",
        "Reference\n",
        "\n",
        "- https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#eq-18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f01781ff",
      "metadata": {
        "id": "f01781ff"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689e849d",
      "metadata": {
        "id": "689e849d",
        "outputId": "d0a8f04b-e0dd-4221-86c4-03da1f15d0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['it', 'convert', 'text', 'document', 'into', 'numerical', 'vector', 'representation'], ['bow', 'model', 'ignores', 'the', 'order', 'and', 'structure', 'of', 'the', 'words', 'in', 'the', 'document', 'and', 'focuses', 'solely', 'on', 'the', 'occurrence', 'and', 'frequency', 'of', 'words'], ['it', 'is', 'used', 'for', 'evulate', 'improtance', 'of', 'a', 'term', 'in', 'a', 'document', 'within', 'a', 'collection', 'or', 'corpus', 'of', 'documents']]\n",
            "vectror shape  (100,)\n",
            "[('occurrence', 0.1822252720594406), ('document', 0.17278888821601868), ('in', 0.16700223088264465), ('is', 0.15654872357845306), ('model', 0.1330449879169464), ('collection', 0.11322769522666931), ('it', 0.11128198355436325), ('convert', 0.10946954041719437), ('ignores', 0.09724971652030945), ('within', 0.0907229632139206)]\n",
            "shape of sentence vector-  (100,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 0.00502373,  0.00104713,  0.00228018,  0.00050057, -0.00067506,\n",
              "       -0.00108926, -0.00096536,  0.00203213, -0.00446166, -0.00252412])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "texts = ['It convert text document into numerical vector representation',\n",
        "        'BoW model ignores the order and structure of the words in the document and focuses solely on the occurrence and frequency of words',\n",
        "        'It is used for evulate improtance of a term in a document within a collection or corpus of documents']\n",
        "\n",
        "#Preprocess Text Data\n",
        "sentence = [i.lower().split() for i in texts]\n",
        "print(sentence)\n",
        "word2vec_model = Word2Vec(sentence,vector_size=100,window=7, min_count=1 ,workers=3,)\n",
        "# size: The dimensionality of the word vectors\n",
        "# window: The maximum distance between the target word and its context words within a sentence\n",
        "# min_count: The minimum frequency count of words.\n",
        "print('vectror shape ' ,word2vec_model.wv['text'].shape)\n",
        "# top 10 similar word of text\n",
        "print(word2vec_model.wv.most_similar('text'))\n",
        "\n",
        "# sentence to vector  -> Take  Average of Word2Vec vectors\n",
        "sentence1 = 'It convert text to document'\n",
        "vec = np.zeros(100,)\n",
        "for word in sentence1.lower().split():\n",
        "    try:\n",
        "        vec += word2vec_model.wv[word]\n",
        "    except:\n",
        "        pass\n",
        "vec /= len(sentence1.split())\n",
        "print(\"shape of sentence vector- \", vec.shape)\n",
        "vec[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e06d23d",
      "metadata": {
        "id": "0e06d23d"
      },
      "source": [
        "## Pre-trained Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577faa7c",
      "metadata": {
        "id": "577faa7c"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#preparing vocabulary\n",
        "tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#converting text into integer sequences\n",
        "x_tr_seq  = tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding to prepare sequences of same length\n",
        "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\n",
        "x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776b9506",
      "metadata": {
        "id": "776b9506"
      },
      "outputs": [],
      "source": [
        "#deep learning library\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "#embedding layer\n",
        "model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n",
        "\n",
        "#lstm layer\n",
        "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
        "\n",
        "#Global Maxpooling\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#Dense Layer\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "#Add loss function, metrics, optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "\n",
        "#Adding callbacks\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
        "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7474b6",
      "metadata": {
        "id": "cd7474b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ae254a",
      "metadata": {
        "id": "07ae254a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ec2739",
      "metadata": {
        "id": "d6ec2739"
      },
      "outputs": [],
      "source": [
        "Word2Vec: Developed by Google, Word2Vec provides word embeddings that capture semantic relationships and contextual information. Two popular architectures of Word2Vec are Continuous Bag-of-Words (CBOW) and Skip-gram. The pre-trained Word2Vec embeddings are available for various languages.\n",
        "\n",
        "GloVe (Global Vectors for Word Representation): GloVe is a word embedding model that combines global statistics of word co-occurrences with local context windows. GloVe embeddings capture semantic and syntactic relationships between words. Pre-trained GloVe embeddings are available in different dimensions, trained on various corpora.\n",
        "\n",
        "FastText: FastText, developed by Facebook AI Research, extends Word2Vec by considering word substructures (character n-grams) in addition to whole words. FastText embeddings are effective for handling out-of-vocabulary words and capturing morphological information. Pre-trained FastText embeddings are available in different languages and dimensions.\n",
        "\n",
        "ELMo (Embeddings from Language Models): ELMo embeddings are based on deep contextualized word representations. These embeddings capture word meanings in context, allowing models to leverage contextual information effectively. ELMo embeddings are trained on large-scale language modeling tasks.\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers): BERT is a state-of-the-art transformer-based model that captures deep contextualized word representations. BERT embeddings consider the bidirectional context of words, leading to rich contextual information. Pre-trained BERT models are available in different sizes and variations, such as BERT-base and BERT-large.\n",
        "\n",
        "ULMFiT (Universal Language Model Fine-tuning): ULMFiT is a transfer learning approach that uses pre-training on a large corpus and fine-tuning on task-specific data. ULMFiT embeddings capture syntactic and semantic information and have been successful in various NLP tasks.\n",
        "\n",
        "Transformer-based Models (e.g., GPT, GPT-2, T5): Transformer-based models, such as OpenAI's GPT (Generative Pre-trained Transformer) and GPT-2, as well as Google's T5 (Text-to-Text Transfer Transformer), have also been used to obtain high-quality pre-trained word embeddings. These models capture deep contextualized representations and have achieved state-of-the-art performance in many NLP tasks.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}